{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "# os.environ['TF_CPP_MIN_LOG_LEVEL'] = 2\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 614 files belonging to 1 classes.\n",
      "Model: \"sequential_15\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_21 (Conv2D)          (None, 32, 32, 64)        3136      \n",
      "                                                                 \n",
      " leaky_re_lu_61 (LeakyReLU)  (None, 32, 32, 64)        0         \n",
      "                                                                 \n",
      " conv2d_22 (Conv2D)          (None, 16, 16, 128)       131200    \n",
      "                                                                 \n",
      " leaky_re_lu_62 (LeakyReLU)  (None, 16, 16, 128)       0         \n",
      "                                                                 \n",
      " conv2d_23 (Conv2D)          (None, 8, 8, 128)         262272    \n",
      "                                                                 \n",
      " leaky_re_lu_63 (LeakyReLU)  (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 8192)              0         \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 8192)              0         \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 1)                 8193      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 404,801\n",
      "Trainable params: 404,801\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "dataset = keras.preprocessing.image_dataset_from_directory(\n",
    "    directory='faces_6class/all', label_mode=None, image_size=(64, 64),\n",
    "    batch_size = 8, shuffle=True\n",
    ").map(lambda x: (x/255.0 - 0.5)*2)\n",
    "\n",
    "\n",
    "discriminator = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(64, 64, 3)),\n",
    "        layers.Conv2D(64, kernel_size=4, strides=2, padding='same'),\n",
    "        layers.LeakyReLU(0.2),\n",
    "        layers.Conv2D(128, kernel_size=4, strides=2, padding='same'),\n",
    "        layers.LeakyReLU(0.2),\n",
    "        layers.Conv2D(128, kernel_size=4, strides=2, padding='same'),\n",
    "        layers.LeakyReLU(0.2),\n",
    "        layers.Flatten(),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(1, activation='sigmoid'),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(discriminator.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_16 (Dense)            (None, 8192)              1056768   \n",
      "                                                                 \n",
      " reshape_12 (Reshape)        (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " conv2d_transpose_52 (Conv2D  (None, 16, 16, 128)      262272    \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " leaky_re_lu_64 (LeakyReLU)  (None, 16, 16, 128)       0         \n",
      "                                                                 \n",
      " conv2d_transpose_53 (Conv2D  (None, 32, 32, 256)      524544    \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " leaky_re_lu_65 (LeakyReLU)  (None, 32, 32, 256)       0         \n",
      "                                                                 \n",
      " conv2d_transpose_54 (Conv2D  (None, 64, 64, 512)      2097664   \n",
      " Transpose)                                                      \n",
      "                                                                 \n",
      " leaky_re_lu_66 (LeakyReLU)  (None, 64, 64, 512)       0         \n",
      "                                                                 \n",
      " conv2d_24 (Conv2D)          (None, 64, 64, 3)         38403     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,979,651\n",
      "Trainable params: 3,979,651\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "latent_dim = 128\n",
    "\n",
    "generator = keras.Sequential(\n",
    "    [\n",
    "        layers.Input(shape=(latent_dim,)),\n",
    "        layers.Dense(8*8*128),\n",
    "        layers.Reshape((8, 8, 128)),\n",
    "        layers.Conv2DTranspose(128, kernel_size=4, strides=2, padding='same'),\n",
    "        layers.LeakyReLU(0.2),\n",
    "        layers.Conv2DTranspose(256, kernel_size=4, strides=2, padding='same'),\n",
    "        layers.LeakyReLU(0.2),\n",
    "        layers.Conv2DTranspose(512, kernel_size=4, strides=2, padding='same'),\n",
    "        layers.LeakyReLU(0.2),\n",
    "        layers.Conv2D(3, kernel_size=5, padding='same', activation='tanh'),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(generator.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mapping: no known devices.\n"
     ]
    }
   ],
   "source": [
    "sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77/77 [02:56<00:00,  2.30s/it]\n",
      " 86%|████████▌ | 66/77 [02:26<00:24,  2.22s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Codoush\\Desktop\\Currently Working\\GAN\\GAN.ipynb Cell 4\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Codoush/Desktop/Currently%20Working/GAN/GAN.ipynb#W3sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     output \u001b[39m=\u001b[39m discriminator(fake)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Codoush/Desktop/Currently%20Working/GAN/GAN.ipynb#W3sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     loss_gen \u001b[39m=\u001b[39m loss_fn(tf\u001b[39m.\u001b[39mones(batch_size, \u001b[39m1\u001b[39m), output)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Codoush/Desktop/Currently%20Working/GAN/GAN.ipynb#W3sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m grads \u001b[39m=\u001b[39m gen_tape\u001b[39m.\u001b[39;49mgradient(loss_gen, generator\u001b[39m.\u001b[39;49mtrainable_weights)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Codoush/Desktop/Currently%20Working/GAN/GAN.ipynb#W3sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m opt_gen\u001b[39m.\u001b[39mapply_gradients(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Codoush/Desktop/Currently%20Working/GAN/GAN.ipynb#W3sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m     \u001b[39mzip\u001b[39m(grads, generator\u001b[39m.\u001b[39mtrainable_weights)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Codoush/Desktop/Currently%20Working/GAN/GAN.ipynb#W3sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Codoush\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py:1100\u001b[0m, in \u001b[0;36mGradientTape.gradient\u001b[1;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[0;32m   1094\u001b[0m   output_gradients \u001b[39m=\u001b[39m (\n\u001b[0;32m   1095\u001b[0m       composite_tensor_gradient\u001b[39m.\u001b[39mget_flat_tensors_for_gradients(\n\u001b[0;32m   1096\u001b[0m           output_gradients))\n\u001b[0;32m   1097\u001b[0m   output_gradients \u001b[39m=\u001b[39m [\u001b[39mNone\u001b[39;00m \u001b[39mif\u001b[39;00m x \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m ops\u001b[39m.\u001b[39mconvert_to_tensor(x)\n\u001b[0;32m   1098\u001b[0m                       \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m output_gradients]\n\u001b[1;32m-> 1100\u001b[0m flat_grad \u001b[39m=\u001b[39m imperative_grad\u001b[39m.\u001b[39;49mimperative_grad(\n\u001b[0;32m   1101\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tape,\n\u001b[0;32m   1102\u001b[0m     flat_targets,\n\u001b[0;32m   1103\u001b[0m     flat_sources,\n\u001b[0;32m   1104\u001b[0m     output_gradients\u001b[39m=\u001b[39;49moutput_gradients,\n\u001b[0;32m   1105\u001b[0m     sources_raw\u001b[39m=\u001b[39;49mflat_sources_raw,\n\u001b[0;32m   1106\u001b[0m     unconnected_gradients\u001b[39m=\u001b[39;49munconnected_gradients)\n\u001b[0;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_persistent:\n\u001b[0;32m   1109\u001b[0m   \u001b[39m# Keep track of watched variables before setting tape to None\u001b[39;00m\n\u001b[0;32m   1110\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_watched_variables \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tape\u001b[39m.\u001b[39mwatched_variables()\n",
      "File \u001b[1;32mc:\\Users\\Codoush\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\imperative_grad.py:67\u001b[0m, in \u001b[0;36mimperative_grad\u001b[1;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[0;32m     64\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m     65\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mUnknown value for unconnected_gradients: \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m unconnected_gradients)\n\u001b[1;32m---> 67\u001b[0m \u001b[39mreturn\u001b[39;00m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_TapeGradient(\n\u001b[0;32m     68\u001b[0m     tape\u001b[39m.\u001b[39;49m_tape,  \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m     69\u001b[0m     target,\n\u001b[0;32m     70\u001b[0m     sources,\n\u001b[0;32m     71\u001b[0m     output_gradients,\n\u001b[0;32m     72\u001b[0m     sources_raw,\n\u001b[0;32m     73\u001b[0m     compat\u001b[39m.\u001b[39;49mas_str(unconnected_gradients\u001b[39m.\u001b[39;49mvalue))\n",
      "File \u001b[1;32mc:\\Users\\Codoush\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py:157\u001b[0m, in \u001b[0;36m_gradient_function\u001b[1;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[0;32m    155\u001b[0m     gradient_name_scope \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m forward_pass_name_scope \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    156\u001b[0m   \u001b[39mwith\u001b[39;00m ops\u001b[39m.\u001b[39mname_scope(gradient_name_scope):\n\u001b[1;32m--> 157\u001b[0m     \u001b[39mreturn\u001b[39;00m grad_fn(mock_op, \u001b[39m*\u001b[39;49mout_grads)\n\u001b[0;32m    158\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    159\u001b[0m   \u001b[39mreturn\u001b[39;00m grad_fn(mock_op, \u001b[39m*\u001b[39mout_grads)\n",
      "File \u001b[1;32mc:\\Users\\Codoush\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\ops\\nn_grad.py:577\u001b[0m, in \u001b[0;36m_Conv2DGrad\u001b[1;34m(op, grad)\u001b[0m\n\u001b[0;32m    568\u001b[0m shape_0, shape_1 \u001b[39m=\u001b[39m array_ops\u001b[39m.\u001b[39mshape_n([op\u001b[39m.\u001b[39minputs[\u001b[39m0\u001b[39m], op\u001b[39m.\u001b[39minputs[\u001b[39m1\u001b[39m]])\n\u001b[0;32m    570\u001b[0m \u001b[39m# We call the gen_nn_ops backprop functions instead of nn_ops backprop\u001b[39;00m\n\u001b[0;32m    571\u001b[0m \u001b[39m# functions for performance reasons in Eager mode. gen_nn_ops functions take a\u001b[39;00m\n\u001b[0;32m    572\u001b[0m \u001b[39m# `explicit_paddings` parameter, but nn_ops functions do not. So if we were\u001b[39;00m\n\u001b[0;32m    573\u001b[0m \u001b[39m# to use the nn_ops functions, we would have to convert `padding` and\u001b[39;00m\n\u001b[0;32m    574\u001b[0m \u001b[39m# `explicit_paddings` into a single `padding` parameter, increasing overhead\u001b[39;00m\n\u001b[0;32m    575\u001b[0m \u001b[39m# in Eager mode.\u001b[39;00m\n\u001b[0;32m    576\u001b[0m \u001b[39mreturn\u001b[39;00m [\n\u001b[1;32m--> 577\u001b[0m     gen_nn_ops\u001b[39m.\u001b[39;49mconv2d_backprop_input(\n\u001b[0;32m    578\u001b[0m         shape_0,\n\u001b[0;32m    579\u001b[0m         op\u001b[39m.\u001b[39;49minputs[\u001b[39m1\u001b[39;49m],\n\u001b[0;32m    580\u001b[0m         grad,\n\u001b[0;32m    581\u001b[0m         dilations\u001b[39m=\u001b[39;49mdilations,\n\u001b[0;32m    582\u001b[0m         strides\u001b[39m=\u001b[39;49mstrides,\n\u001b[0;32m    583\u001b[0m         padding\u001b[39m=\u001b[39;49mpadding,\n\u001b[0;32m    584\u001b[0m         explicit_paddings\u001b[39m=\u001b[39;49mexplicit_paddings,\n\u001b[0;32m    585\u001b[0m         use_cudnn_on_gpu\u001b[39m=\u001b[39;49muse_cudnn_on_gpu,\n\u001b[0;32m    586\u001b[0m         data_format\u001b[39m=\u001b[39;49mdata_format),\n\u001b[0;32m    587\u001b[0m     gen_nn_ops\u001b[39m.\u001b[39mconv2d_backprop_filter(\n\u001b[0;32m    588\u001b[0m         op\u001b[39m.\u001b[39minputs[\u001b[39m0\u001b[39m],\n\u001b[0;32m    589\u001b[0m         shape_1,\n\u001b[0;32m    590\u001b[0m         grad,\n\u001b[0;32m    591\u001b[0m         dilations\u001b[39m=\u001b[39mdilations,\n\u001b[0;32m    592\u001b[0m         strides\u001b[39m=\u001b[39mstrides,\n\u001b[0;32m    593\u001b[0m         padding\u001b[39m=\u001b[39mpadding,\n\u001b[0;32m    594\u001b[0m         explicit_paddings\u001b[39m=\u001b[39mexplicit_paddings,\n\u001b[0;32m    595\u001b[0m         use_cudnn_on_gpu\u001b[39m=\u001b[39muse_cudnn_on_gpu,\n\u001b[0;32m    596\u001b[0m         data_format\u001b[39m=\u001b[39mdata_format)\n\u001b[0;32m    597\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Users\\Codoush\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py:1239\u001b[0m, in \u001b[0;36mconv2d_backprop_input\u001b[1;34m(input_sizes, filter, out_backprop, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[0;32m   1237\u001b[0m \u001b[39mif\u001b[39;00m tld\u001b[39m.\u001b[39mis_eager:\n\u001b[0;32m   1238\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1239\u001b[0m     _result \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_FastPathExecute(\n\u001b[0;32m   1240\u001b[0m       _ctx, \u001b[39m\"\u001b[39;49m\u001b[39mConv2DBackpropInput\u001b[39;49m\u001b[39m\"\u001b[39;49m, name, input_sizes, \u001b[39mfilter\u001b[39;49m, out_backprop,\n\u001b[0;32m   1241\u001b[0m       \u001b[39m\"\u001b[39;49m\u001b[39mstrides\u001b[39;49m\u001b[39m\"\u001b[39;49m, strides, \u001b[39m\"\u001b[39;49m\u001b[39muse_cudnn_on_gpu\u001b[39;49m\u001b[39m\"\u001b[39;49m, use_cudnn_on_gpu, \u001b[39m\"\u001b[39;49m\u001b[39mpadding\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   1242\u001b[0m       padding, \u001b[39m\"\u001b[39;49m\u001b[39mexplicit_paddings\u001b[39;49m\u001b[39m\"\u001b[39;49m, explicit_paddings, \u001b[39m\"\u001b[39;49m\u001b[39mdata_format\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   1243\u001b[0m       data_format, \u001b[39m\"\u001b[39;49m\u001b[39mdilations\u001b[39;49m\u001b[39m\"\u001b[39;49m, dilations)\n\u001b[0;32m   1244\u001b[0m     \u001b[39mreturn\u001b[39;00m _result\n\u001b[0;32m   1245\u001b[0m   \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "opt_gen = keras.optimizers.Adam(1e-4)\n",
    "opt_disc = keras.optimizers.Adam(1e-4)\n",
    "loss_fn = keras.losses.BinaryCrossentropy()\n",
    "\n",
    "for epoch in range(500):\n",
    "    for idx, real in enumerate(tqdm(dataset)):\n",
    "        batch_size = real.shape[0]\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, latent_dim))\n",
    "        fake = generator(random_latent_vectors)\n",
    "\n",
    "        if idx % 100 == 0:\n",
    "            img = keras.preprocessing.image.array_to_img(fake[0])\n",
    "            img.save(f\"generated/generated_img{epoch}_{idx}_.png\")\n",
    "\n",
    "        ### Train Discriminator: max log(D(x)) + log(1 - D(G(z))\n",
    "        with tf.GradientTape() as disc_tape:\n",
    "            loss_disc_real = loss_fn(tf.ones((batch_size, 1)), discriminator(real))\n",
    "            loss_disc_fake = loss_fn(tf.zeros(batch_size, 1), discriminator(fake))\n",
    "            loss_disc = (loss_disc_real + loss_disc_fake)/2\n",
    "\n",
    "        grads = disc_tape.gradient(loss_disc, discriminator.trainable_weights)\n",
    "        opt_disc.apply_gradients(\n",
    "            zip(grads, discriminator.trainable_weights)\n",
    "        )\n",
    "\n",
    "        ### Train Generator min log(1 - D(G(z)) <-> max log(D(G(z))\n",
    "        with tf.GradientTape() as gen_tape:\n",
    "            fake = generator(random_latent_vectors)\n",
    "            output = discriminator(fake)\n",
    "            loss_gen = loss_fn(tf.ones(batch_size, 1), output)\n",
    "\n",
    "        grads = gen_tape.gradient(loss_gen, generator.trainable_weights)\n",
    "        opt_gen.apply_gradients(\n",
    "            zip(grads, generator.trainable_weights)\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "generator.save_weights('gen_w')\n",
    "discriminator.save_weights('dis_w')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ee290b7b9a1da6c4b4af8cc7223e12663196708cb755e158439b8107ef5d26a2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
